{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edb2be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d89ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27f795",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8595a937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0581e2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2cb6e1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  user_input = input(\"Enter the news you want to search : \")\n",
    "#     input_countainer  = driver.find_elements(By.TAG_NAME, 'input')[0]\n",
    "#     input_countainer.send_keys(user_input)\n",
    "#     input_countainer.send_keys(Keys.ENTER)\n",
    "#     time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1a9c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d739b3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# from selenium.webdriver.common.by import By\n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# from selenium.webdriver.chrome.options import Options\n",
    "# import pandas as pd\n",
    "# import time\n",
    "# import os\n",
    "\n",
    "\n",
    "# def web_scraper_chrome(query):\n",
    "\n",
    "\n",
    "\n",
    "#     driver = webdriver.Chrome()\n",
    "\n",
    "#     result = []\n",
    "#     try:\n",
    "#         driver.get(\"https://news.google.com/\")\n",
    "        \n",
    "#         time.sleep(2)\n",
    "\n",
    "#         # Search \n",
    "#         input_countainer  = driver.find_elements(By.TAG_NAME, 'input')[0]\n",
    "#         input_countainer.send_keys(query)\n",
    "#         input_countainer.send_keys(Keys.ENTER)\n",
    "\n",
    "#         time.sleep(2)  \n",
    "\n",
    "#         # Fetch card + articles of the news \n",
    "#         card = driver.find_elements(By.CLASS_NAME,'UW0SDc')\n",
    "#         article = card[0].find_elements(By.TAG_NAME,\"article\")\n",
    "        \n",
    "#         for i , art in enumerate(article[:2], start=1):\n",
    "#             # headline gadhering \n",
    "#             headline_text = art.text\n",
    "\n",
    "#             # Links gadhering \n",
    "#             anchors_links = art.find_elements(By.CLASS_NAME,\"WwrzSb\")\n",
    "#             news_links = anchors_links[0].get_attribute(\"href\")\n",
    "#             # storing headiers and results \n",
    "#             result.append({\"Headlines\": headline_text, \"links\":news_links})\n",
    "\n",
    "#         # paragraphs gadhering (opening link and fetching the text from that)\n",
    "\n",
    "#         for item in result:\n",
    "#             driver.get(item['links'])\n",
    "#             time.sleep(2)\n",
    "#             paragraphs = driver.find_elements(By.TAG_NAME, \"p\")\n",
    "#             article_text = \" \".join([p.text for p in paragraphs if p.text.strip() !=\"\"])\n",
    "#             item['context'] = article_text\n",
    "          \n",
    "\n",
    "\n",
    "#             # Data storing in csv formatt \n",
    "#             df = pd.DataFrame([{\n",
    "#                 \"Headline\": item['Headlines'],\n",
    "#                 \"Context\": item['context'],\n",
    "#                 \"Link\": item['links']\n",
    "#             }])\n",
    "#             print(df)\n",
    "#             df.to_csv(\"news_data.csv\",mode=\"a\",index=False,header=not os.path.exists('news_data.csv'))\n",
    "\n",
    "#             # print(f\"Hedlined {item['Headlines']}\\n Link : {item['links']}\\n Context : {article_text[:1000]}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "#     finally:\n",
    "#         driver.quit()\n",
    "#     return result\n",
    "\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "add05317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web_scraper_chrome('India')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bc3f00ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data \n",
    "# preprocesing P1 Textpreprocessing , Lower cace , and implementing regex on whole text . \n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import re \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report , confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fbb70aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# news_api_key  = os.getenv(\"news_api\")\n",
    "csv_file = os.getenv(\"path_to_csv_file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1a8f1213",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import requests\n",
    "NEWS_API_KEY = os.getenv('news_api')\n",
    "\n",
    "def fetch_news(query,page_size=10):\n",
    "\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&language=en&pageSize={page_size}&apiKey={NEWS_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    data = response.json()\n",
    "    articles = data[\"articles\"]\n",
    "    \n",
    "    df = pd.DataFrame([{\n",
    "            'Heading':art.get('title'),\n",
    "            'Context': (art.get('description')+ (art.get('content'))),\n",
    "            'Link': (art.get('url'))\n",
    "\n",
    "        }for art in articles])\n",
    "    df.to_csv(\"news_data.csv\",mode=\"a\",index=False,header=not os.path.exists('news_data.csv'))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f69c2e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Heading</th>\n",
       "      <th>Context</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple is reportedly making more of its new iPh...</td>\n",
       "      <td>Apple is manufacturing more of its iPhone 17 p...</td>\n",
       "      <td>https://www.theverge.com/news/761582/apple-iph...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>India eyes quick EU trade deal as Trump pushes...</td>\n",
       "      <td>The talks between India and EU come as Trump u...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c3e73glng07o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Nepal turmoil adds to India's woes in South Asia</td>\n",
       "      <td>Any instability in the country is a cause of c...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c8xrqld50dko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why India tops the list of abandoned sailors</td>\n",
       "      <td>Around 900 Indian sailors were abandoned on sh...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c3r4nr2zy2do</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A Case Study in AI Overstatement: Builder.ai</td>\n",
       "      <td>Creditors seized its accounts, it may have bee...</td>\n",
       "      <td>https://gizmodo.com/builder-ai-collapses-20006...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Make and spend in India, urges Modi, as Trump'...</td>\n",
       "      <td>With Trump's 50% tariffs about to kick in, Mod...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c5ykznn158qo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>How researchers recreated faces of 2,500-year-...</td>\n",
       "      <td>The skulls and other goods were found in massi...</td>\n",
       "      <td>https://www.bbc.com/news/articles/c9d061dv36lo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Apple Announces Fourth Store in India</td>\n",
       "      <td>Apple is launching a new retail store location...</td>\n",
       "      <td>https://www.macrumors.com/2025/08/26/apple-ann...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Cross left out of England's World Cup squad</td>\n",
       "      <td>Seamer Kate Cross misses out on selection for ...</td>\n",
       "      <td>https://www.bbc.com/sport/cricket/articles/cn5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Floods kill 30 and submerge 1,400 villages in ...</td>\n",
       "      <td>More than 354,000 people have been affected by...</td>\n",
       "      <td>https://www.bbc.com/news/articles/cvg4g95y7pjo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Heading  \\\n",
       "0  Apple is reportedly making more of its new iPh...   \n",
       "1  India eyes quick EU trade deal as Trump pushes...   \n",
       "2   Nepal turmoil adds to India's woes in South Asia   \n",
       "3       Why India tops the list of abandoned sailors   \n",
       "4       A Case Study in AI Overstatement: Builder.ai   \n",
       "5  Make and spend in India, urges Modi, as Trump'...   \n",
       "6  How researchers recreated faces of 2,500-year-...   \n",
       "7              Apple Announces Fourth Store in India   \n",
       "8        Cross left out of England's World Cup squad   \n",
       "9  Floods kill 30 and submerge 1,400 villages in ...   \n",
       "\n",
       "                                             Context  \\\n",
       "0  Apple is manufacturing more of its iPhone 17 p...   \n",
       "1  The talks between India and EU come as Trump u...   \n",
       "2  Any instability in the country is a cause of c...   \n",
       "3  Around 900 Indian sailors were abandoned on sh...   \n",
       "4  Creditors seized its accounts, it may have bee...   \n",
       "5  With Trump's 50% tariffs about to kick in, Mod...   \n",
       "6  The skulls and other goods were found in massi...   \n",
       "7  Apple is launching a new retail store location...   \n",
       "8  Seamer Kate Cross misses out on selection for ...   \n",
       "9  More than 354,000 people have been affected by...   \n",
       "\n",
       "                                                Link  \n",
       "0  https://www.theverge.com/news/761582/apple-iph...  \n",
       "1     https://www.bbc.com/news/articles/c3e73glng07o  \n",
       "2     https://www.bbc.com/news/articles/c8xrqld50dko  \n",
       "3     https://www.bbc.com/news/articles/c3r4nr2zy2do  \n",
       "4  https://gizmodo.com/builder-ai-collapses-20006...  \n",
       "5     https://www.bbc.com/news/articles/c5ykznn158qo  \n",
       "6     https://www.bbc.com/news/articles/c9d061dv36lo  \n",
       "7  https://www.macrumors.com/2025/08/26/apple-ann...  \n",
       "8  https://www.bbc.com/sport/cricket/articles/cn5...  \n",
       "9     https://www.bbc.com/news/articles/cvg4g95y7pjo  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_corpus = fetch_news('India')\n",
    "raw_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04bb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ce1064ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt_tab')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b47b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Source          Author  \\\n",
      "0        stgnews  Bridger Palmer   \n",
      "1  Zimbabwe Mail  Staff Reporter   \n",
      "2      4-traders             NaN   \n",
      "\n",
      "                                               Title  \\\n",
      "0  Pine View High teacher wins Best in State awar...   \n",
      "1  Businesses Face Financial Strain Amid Liquidit...   \n",
      "2  Musk donates to super pac working to elect Tru...   \n",
      "\n",
      "                                         Description  \\\n",
      "0  ST. GEORGE — Kaitlyn Larson, a first-year teac...   \n",
      "1  Harare, Zimbabwe – Local businesses are grappl...   \n",
      "2  (marketscreener.com) Billionaire Elon Musk has...   \n",
      "\n",
      "                                                 URL  \\\n",
      "0  https://www.stgeorgeutah.com/news/archive/2024...   \n",
      "1  https://www.thezimbabwemail.com/business/busin...   \n",
      "2  https://www.marketscreener.com/business-leader...   \n",
      "\n",
      "                Published At Sentiment      Type  \n",
      "0  2024-07-12T23:45:25+00:00  positive  Business  \n",
      "1  2024-07-12T22:59:42+00:00   neutral  Business  \n",
      "2  2024-07-12T22:52:55+00:00  positive  Business  \n",
      "Index(['Source', 'Author', 'Title', 'Description', 'URL', 'Published At',\n",
      "       'Sentiment', 'Type'],\n",
      "      dtype='object')\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3500 entries, 0 to 3499\n",
      "Data columns (total 8 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Source        3500 non-null   object\n",
      " 1   Author        2512 non-null   object\n",
      " 2   Title         3500 non-null   object\n",
      " 3   Description   3500 non-null   object\n",
      " 4   URL           3500 non-null   object\n",
      " 5   Published At  3500 non-null   object\n",
      " 6   Sentiment     3500 non-null   object\n",
      " 7   Type          3500 non-null   object\n",
      "dtypes: object(8)\n",
      "memory usage: 218.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Data loading \n",
    "data = pd.read_csv(r\"E:\\AI Engineearing\\Projects\\job_finder_proj\\Sentimantal_analysis\\archive\\news_sentiment_analysis.csv\")\n",
    "print(data.head(3))\n",
    "print(data.columns)\n",
    "print(data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6741b3e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f4a4d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Lematizer_function (Text):\n",
    "    lematizer = WordNetLemmatizer()\n",
    "    return lematizer.lemmatize(Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4314e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing (text , Lematizer_function):\n",
    "    \"\"\" this funtion help to preprocess the data \n",
    "        1.Lowercasing\n",
    "        2.Removing non-alphabets\n",
    "        3.Stopword removal\n",
    "        4.Tokenition\n",
    "        5.Lemmatization\n",
    "        6.storing the clean text in to list and return the list \n",
    "    \n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    corpus  = []\n",
    "    for i in text:\n",
    "        try:\n",
    "            # Text lowering\n",
    "            text = i.lower()\n",
    "\n",
    "            # removing non-alphabatic characxters \n",
    "            text = re.sub(r'[^a-zA-Z]',' ',text)\n",
    "            # Tokenization of words \n",
    "            words_tokens = word_tokenize(text,language=\"english\")\n",
    "            # remove stop_words and apply Lematization\n",
    "            clean_word =  [Lematizer_function (word) for word in words_tokens if  not word  in stop_words]\n",
    "\n",
    "            # join the clean word to make sentence \n",
    "            clean_text = ' '.join(clean_word)\n",
    "\n",
    "            # append the clean word to corpus list \n",
    "\n",
    "            corpus.append(clean_text)\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Erroe:\",e)\n",
    "            return corpus.append(\"\")\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8e6a7585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hedlines_preprocessing (text ):\n",
    "    hedlines  = []\n",
    "    for i in text:\n",
    "        try:\n",
    "\n",
    "            # removing non-alphabatic characxters \n",
    "            text = re.sub(r'[^a-zA-Z]',' ',i)\n",
    "            cleaned_text = text.strip().lower()\n",
    "\n",
    "\n",
    "            # append the clean word to hedlines list \n",
    "\n",
    "            hedlines.append(cleaned_text if cleaned_text else \"\")\n",
    "\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"Erroe:\",e)\n",
    "            return hedlines.append(\"\")\n",
    "    return hedlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f33276ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['heading', 'context', 'link']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hedlines_preprocessing(raw_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "27af7183",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = text_preprocessing(data['Description'],Lematizer_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e9a9eb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# def encode_labels(df, column_name, new_column_name):\n",
    "    \n",
    "#     label_encoder = LabelEncoder()\n",
    "#     df[new_column_name] = label_encoder.fit_transform(df[column_name])\n",
    "    \n",
    "#     return df, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5ab3c2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: encode the \"Sentiment\" column\n",
    "# data, le = encode_labels(data, \"Sentiment\", \"labeled_sentiment\")\n",
    "\n",
    "# print(data['labeled_sentiment'].value_counts())\n",
    "# print(\"Classes mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "073bc147",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labled_sentiment\n",
       "2    2134\n",
       "1     789\n",
       "0     577\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labeling the Independet variable\n",
    "lable = LabelEncoder()\n",
    "data['labled_sentiment'] = lable.fit_transform(data['Sentiment'])\n",
    "data['labled_sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b19f088",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = corpus\n",
    "y = data['labled_sentiment'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9ec93f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test , y_train , y_test = train_test_split(x , y ,test_size=0.2,random_state=42,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "dd7894c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_vactor = TfidfVectorizer(max_features = 1000 ,ngram_range=(1,3))\n",
    "x_train_vec = tf_idf_vactor.fit_transform(x_train)\n",
    "x_test_vec = tf_idf_vactor.fit_transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9b8849c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.24      0.07      0.11       115\n",
      "           1       0.49      0.32      0.38       158\n",
      "           2       0.65      0.85      0.74       427\n",
      "\n",
      "    accuracy                           0.60       700\n",
      "   macro avg       0.46      0.41      0.41       700\n",
      "weighted avg       0.54      0.60      0.55       700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model 2 \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "Nav_bias_model = MultinomialNB()\n",
    "Nav_bias_model.fit(x_train_vec,y_train)\n",
    "\n",
    "# evalution\n",
    "y_pred_2 = Nav_bias_model.predict(x_test_vec)\n",
    "print(classification_report(y_test,y_pred_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "afd28939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4cdd7096",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.07      0.01      0.02       115\n",
      "           1       0.45      0.68      0.54       158\n",
      "           2       0.70      0.74      0.72       427\n",
      "\n",
      "    accuracy                           0.60       700\n",
      "   macro avg       0.41      0.47      0.43       700\n",
      "weighted avg       0.54      0.60      0.56       700\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rfc_model = RandomForestClassifier(\n",
    "    max_depth= None, n_estimators= 400 , class_weight= 'balanced', random_state=42)\n",
    "\n",
    "rfc_model.fit(x_train_vec,y_train)\n",
    "\n",
    "# Evaluate \n",
    "rfc_pred = rfc_model.predict(x_test_vec)\n",
    "\n",
    "print(classification_report(y_test,rfc_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "765321d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "890c7e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open ('sentimet_model.pkl', 'wb') as f:\n",
    "    pickle.dump(Nav_bias_model, f)\n",
    "\n",
    "with open ('vector_embeddng.pkl', 'wb') as f:\n",
    "    pickle.dump(tf_idf_vactor, f)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "09d45353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentimental_model_path = os.getenv('sentiment_model_path')\n",
    "# vectors_path = os.getenv('vector_data_path')\n",
    "# csv_path = os.getenv('path_to_csv_file')\n",
    "\n",
    "# # Load the model\n",
    "# with open(sentimental_model_path, \"rb\") as f:\n",
    "#     sentimental_model = pickle.load(f)\n",
    "\n",
    "# # Load the vectorizer\n",
    "# with open(vectors_path, \"rb\") as f:\n",
    "#     vectorizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9e5ed94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentimental_model.predict(x_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e1406412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc61b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "911cc391",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = os.getenv('path_to_csv_file')\n",
    "csv_path\n",
    "# Load the model\n",
    "with open('sentimet_model.pkl', \"rb\") as f:\n",
    "    sentimental_model = pickle.load(f)\n",
    "\n",
    "# Load the vectorizer\n",
    "with open('vector_embeddng.pkl', \"rb\") as f:\n",
    "    vectorizer = pickle.load(f)\n",
    "\n",
    "def data_deployment(csv_path ,text_preprocessing, vectorizer, sentimental_model):\n",
    "    # 1. Load dataset\n",
    "    raw_data = pd.read_csv(csv_path)\n",
    "\n",
    "    # hedline _cleaning \n",
    "    hedlines = hedlines_preprocessing(raw_data['Heading'])\n",
    "\n",
    "    # 2. Preprocess text\n",
    "    preprocessed_data = text_preprocessing(raw_data['Context'], Lematizer_function)\n",
    "\n",
    "    # 3. Vectorize (DON'T fit again, only transform)\n",
    "    raw_data_vec = vectorizer.transform(preprocessed_data)\n",
    "\n",
    "    # 4. Predict\n",
    "    raw_data_prediction = sentimental_model.predict(raw_data_vec)\n",
    "\n",
    "    sentiment_map = {0: 'Negative', 1: 'Neutral', 2: 'Positive'}\n",
    "\n",
    "    # 5. storing it in to the SQLite3 \n",
    "    connection_obj = sqlite3.connect('News_data.db')\n",
    "    cursor_obj = connection_obj.cursor()\n",
    "    cursor_obj.execute(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS news_predictions (\n",
    "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "        Headline TEXT,\n",
    "        Sentence TEXT,\n",
    "        Sentiment TEXT\n",
    "    )\n",
    "    \"\"\")\n",
    "\n",
    "    # 6. Insert Records into data.\n",
    "    for hedline, sentence, pred in zip(hedlines, preprocessed_data, raw_data_prediction):\n",
    "        cursor_obj.execute(\n",
    "            \"INSERT INTO news_predictions (Headline, Sentence, Sentiment) VALUES (?, ?, ?)\",\n",
    "            (hedline, sentence, sentiment_map[pred])\n",
    "        )\n",
    "\n",
    "    connection_obj.commit()\n",
    "    connection_obj.close()\n",
    "    \n",
    "    return f\" {hedline}\\n , {sentence}\\n ,{sentiment_map}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "56aab0f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" floods kill    and submerge       villages in indian state\\n , people affected worst flood since rescue effort continue abhishek deybbc news delhi people affected extreme rain least people died affected incessant heavy rain floo char\\n ,{0: 'Negative', 1: 'Neutral', 2: 'Positive'}\""
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_deployment(csv_path,text_preprocessing,vectorizer ,sentimental_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0029fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "85af3fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_data_from_db(db ='News_data.db'):\n",
    "    con = sqlite3.connect(db)\n",
    "    cur = con.cursor()\n",
    "    statement = '''SELECT * FROM news_predictions'''\n",
    "    cur.execute(statement)\n",
    "    \n",
    "    print(\"All the data\")\n",
    "    output = cur.fetchall()\n",
    "    for row in output:\n",
    "        print(row)\n",
    "\n",
    "    con.commit()\n",
    "    con.close()\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dd5b80bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All the data\n",
      "(1, 'apple is reportedly making more of its new iphones in india instead of china', 'apple manufacturing iphone phone u india instead china first time full lineup new model ship india launch according bloomberg company also working successor iphone ul li li li li li li ul launch apple factory india produce four new iphone variant first time according bloomberg launch apple factory ind char', 'Positive')\n",
      "(2, 'india eyes quick eu trade deal as trump pushes higher tariffs', 'talk india eu come trump urge eu impose tariff india china buying russian oil meryl sebastianbbc news kochi india close finalising large part free trade agreement european union eu indian commerce minister piyush goyal said remark come ah char', 'Positive')\n",
      "(3, 'nepal turmoil adds to india s woes in south asia', 'instability country cause concern india nepal strategic location anbarasan ethirajanglobal affair reporter nepal become third country india immediate neighbourhood see violent uprising topple government recent year prime minister kp char', 'Positive')\n",
      "(4, 'why india tops the list of abandoned sailors', 'around indian sailor abandoned ship across world neyaz farooquee indian crew member anka cargo vessel abandoned ukrainian port since april manas kumar abandoned cargo ship ukrainian water since april char', 'Positive')\n",
      "(5, 'a case study in ai overstatement  builder ai', 'creditor seized account may using engineer india instead ai probe builder ai founder spent money leading collapse back may builder ai high flying startup touted path clearing ai powered app builder filed bankruptcy u culminating spectacular fall become cautionary tal char', 'Positive')\n",
      "(6, 'make and spend in india  urges modi  as trump s     tariffs kick in', 'trump tariff kick modi announced overhaul india indirect tax system modi urged small shop owner business put made india board outside store earlier month indian prime minister narendra modi made promise said diwali gift char', 'Positive')\n",
      "(7, 'how researchers recreated faces of       year old skulls found in india', 'skull good found massive burial urn near keeladi tamil nadu state cherylann mollanreporting keeladi tamil nadu facial reconstruction year old skull excavated kondagai modest sized university lab southern indian state tamil char', 'Neutral')\n",
      "(8, 'apple announces fourth store in india', 'apple launching new retail store location india apple koregaon park set open thursday september p local time apple koregaon park located pune state maharashtra apple already two retail store locati apple launching new retail store location india apple koregaon park set open thursday september p local time apple koregaon park located pune state char', 'Positive')\n",
      "(9, 'cross left out of england s world cup squad', 'seamer kate cross miss selection england squad woman world cup start end september danni wyatt hodge sarah glenn return kate cross left england squad woman world cup india sri lanka batter danni wyatt hodge leg spinner sarah glenn recalled former captain heather char', 'Positive')\n",
      "(10, 'floods kill    and submerge       villages in indian state', 'people affected worst flood since rescue effort continue abhishek deybbc news delhi people affected extreme rain least people died affected incessant heavy rain floo char', 'Positive')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " 'floods kill    and submerge       villages in indian state',\n",
       " 'people affected worst flood since rescue effort continue abhishek deybbc news delhi people affected extreme rain least people died affected incessant heavy rain floo char',\n",
       " 'Positive')"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fetch_data_from_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2205644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "parva_ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
